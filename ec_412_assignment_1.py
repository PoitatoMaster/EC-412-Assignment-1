# -*- coding: utf-8 -*-
"""EC-412 Assignment-1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nkGpukIxBi0gcllHoi2wNdubJiEHm_Hw

Importing libraries
"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.utils import shuffle
import cv2
import os
import glob

"""Model Definition"""

from functools import partial

DefaultConv2D = partial(keras.layers.Conv2D,
                       kernel_size = 3, activation = 'relu', padding = 'same')

model = keras.models.Sequential([
    DefaultConv2D(filters = 32, input_shape = [100, 100, 1]),
    keras.layers.MaxPooling2D(pool_size=3),
    DefaultConv2D(filters= 64),
    keras.layers.MaxPooling2D(pool_size=3),
    keras.layers.Flatten(),
    keras.layers.Dense(units = '128', activation = 'relu'),
    keras.layers.Dropout(0.3),
    keras.layers.Dense(units = '2', activation = 'softmax'),
])

model.summary()

"""Dataset import"""

size = 144

train_data = []
train_labels = []

for per in os.listdir('/home/darthnoobisuke/Downloads/archive/sign_data/train/'):
    for data in glob.glob('/home/darthnoobisuke/Downloads/archive/sign_data/train/'+per+'/*.*'):
        img = cv2.imread(data,cv2.IMREAD_GRAYSCALE)
        img = cv2.resize(img, (size, size))
        train_data.append([img])
        if per[-1]=='g':
            train_labels.append(np.array(1))
        else:
            train_labels.append(np.array(0))

train_data = np.array(train_data)/255.0
train_labels = np.array(train_labels)

test_data = []
test_labels = []

for per in os.listdir('/home/darthnoobisuke/Downloads/archive/sign_data/test/'):
    for data in glob.glob('/home/darthnoobisuke/Downloads/archive/sign_data/test/'+per+'/*.*'):
        img = cv2.imread(data,cv2.IMREAD_GRAYSCALE)
        img = cv2.resize(img, (size, size))
        test_data.append([img])
        if per[-1]=='g':
            test_labels.append(1)
        else:
            test_labels.append(0)

test_data = np.array(test_data)/255.0
test_labels = np.array(test_labels)

train_labels = tf.keras.utils.to_categorical(train_labels, dtype="float32")

train_data = train_data.reshape(-1, size,size, 1)
test_data = test_data.reshape(-1, size,size, 1)

train_data,train_labels = shuffle(train_data,train_labels)
test_data,test_labels = shuffle(test_data,test_labels)

"""Compile and training the model"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
import datetime

opt = keras.optimizers.Adam(learning_rate = 0.0005)
model.compile(loss = "categorical_crossentropy", optimizer = opt, metrics = ["accuracy"])

log_dir = "logs/fit/Sign_Check" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,
                              patience=6, min_lr=0.001)
    
early_stopping_cb = keras.callbacks.EarlyStopping(patience=9,
                                                  restore_best_weights=True)

checkpoint_cb = keras.callbacks.ModelCheckpoint("Sign_Check.h5",save_best_only=True)

history = model.fit(train_data, train_labels,
                    epochs = 60,
                    batch_size = 64,
                    validation_split=.3, 
                    callbacks = [reduce_lr, early_stopping_cb, tensorboard_callback, checkpoint_cb])

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs/fit

pred = model.predict(test_data)
predict =np.argmax(pred,axis=1)

from sklearn.metrics import accuracy_score
accuracy_score(predict , test_labels)

from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score

f1_score(predict, test_labels)

recall_score(predict, test_labels)

precision_score(predict,test_labels)

confusion_matrix(predict,test_labels)

